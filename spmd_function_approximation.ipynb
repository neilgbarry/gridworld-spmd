{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Y_r1QB3C7Gc"
      },
      "outputs": [],
      "source": [
        "\"\"\" Basic stochastic PMD \"\"\"\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" White box MDPs (wbmdp) defined five-tuple M=(S,A,c,P,gamma) \"\"\"\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "\n",
        "import sklearn\n",
        "import sklearn.pipeline\n",
        "import sklearn.kernel_approximation\n",
        "import sklearn.linear_model\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "\n",
        "TOL = 1e-10\n",
        "\n",
        "# Right (0), Down (1), Left (2), Up (3)\n",
        "DIRS = [(1,0), (0,1), (-1,0), (0,-1)]\n",
        "\n",
        "class MDPModel():\n",
        "    \"\"\" Base MDP class \"\"\"\n",
        "    def __init__(self, n_states, n_actions, c, P, gamma, rho=None, seed=None):\n",
        "        assert len(c.shape) == 2, \"Input cost vector c must be a 2-D vector, recieved %d dimensions\" % len(c.shape)\n",
        "        assert len(P.shape) == 3, \"Input cost vector c must be a 3-D tensor, recieved %d dimensions\" % len(P.shape)\n",
        "\n",
        "        assert c.shape[0] == n_states, \"1st dimension of c must equal n_states=%d, was instead %d\" % (n_states, c.shape[0])\n",
        "        assert c.shape[1] == n_actions, \"2nd dimension of c must equal n_actions=%d, was instead %d\" % (n_actions, c.shape[1])\n",
        "        assert P.shape[0] == n_states, \"1st dimension of P must equal n_states=%d, was instead %d\" % (n_states, P.shape[0])\n",
        "        assert P.shape[1] == n_states, \"2nd dimension of P must equal n_states=%d, was instead %d\" % (n_states, P.shape[1])\n",
        "        assert P.shape[2] == n_actions, \"3rd dimension of P must equal n_actions=%d, was instead %d\" % (n_actions, P.shape[2])\n",
        "        assert 0 < gamma < 1, \"Input discount gamma must be (0,1), recieved %f\" % gamma\n",
        "\n",
        "        assert 1-TOL <= np.min(np.sum(P, axis=0)), \\\n",
        "            \"P is not stochastic, recieved a sum of %.2f at (s,a)=(%d,%d)\" % ( \\\n",
        "                np.min(np.sum(P, axis=0)), \\\n",
        "                np.where(1-TOL > np.sum(P, axis=0))[0][0], \\\n",
        "                np.where(1-TOL > np.sum(P, axis=0))[1][0], \\\n",
        "            )\n",
        "        assert np.max(np.sum(P, axis=0)) <= 1+TOL, \\\n",
        "            \"P is not stochastic, recieved a sum of %.2f at (s,a)=(%d,%d)\" % ( \\\n",
        "                np.max(np.sum(P, axis=0)), \\\n",
        "                np.where(1+TOL < np.sum(P, axis=0))[0][0], \\\n",
        "                np.where(1+TOL < np.sum(P, axis=0))[1][0], \\\n",
        "            )\n",
        "\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.c = c\n",
        "        self.P = P\n",
        "        self.gamma = gamma\n",
        "        if rho is None:\n",
        "            rho = np.ones(self.n_states, dtype=float)/self.n_states\n",
        "        self.rho = rho\n",
        "\n",
        "        # initialize a\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.s = self.rng.integers(0, self.n_states)\n",
        "\n",
        "        # initialize rbf for solving with linear function approx\n",
        "        self.init_linear = False\n",
        "\n",
        "    def get_advantage(self, pi):\n",
        "        assert pi.shape[0] == self.n_actions, \"1st dimension of pi must equal n_actions=%d, was instead %d\" % (self.n_actions, pi.shape[0])\n",
        "        assert pi.shape[1] == self.n_states, \"2nd dimension of pi must equal n_states=%d, was instead %d\" % (self.n_states, pi.shape[1])\n",
        "\n",
        "        # sum over actions (p=s' next state, s curr state, a action)\n",
        "        P_pi = np.einsum('psa,as->ps', self.P, pi)\n",
        "        c_pi = np.einsum('sa,as->s', self.c, pi)\n",
        "\n",
        "        # (I-gamma*(P^pi)')V = c^pi\n",
        "        V_pi = la.solve(np.eye(self.n_states) - self.gamma*P_pi.T, c_pi)\n",
        "        Q_pi = self.c + self.gamma*np.einsum('psa,p->sa', self.P, V_pi)\n",
        "        psi = Q_pi - np.outer(V_pi, np.ones(self.n_actions))\n",
        "\n",
        "        return (psi, V_pi)\n",
        "\n",
        "    def estimate_advantage_generative_slow(self, pi, N, T):\n",
        "        \"\"\"\n",
        "        :param N: number of Monte Carlo simulations to run per state-action pair\n",
        "        :param T: duration to for each Monte Carlo simulation\n",
        "        \"\"\"\n",
        "        Q = np.zeros((self.n_states, self.n_actions), dtype=float)\n",
        "\n",
        "        for s in range(self.n_states):\n",
        "            for a in range(self.n_actions):\n",
        "                costs = 0.\n",
        "                for i in range(N):\n",
        "                    s_t = s\n",
        "                    a_t = a\n",
        "                    for t in range(T):\n",
        "                        Q[s,a] += self.gamma**t * self.c[s_t,a_t]\n",
        "                        s_t_next = self.rng.choice(self.P.shape[0], p=self.P[:,s_t,a_t])\n",
        "                        a_t = self.rng.choice(pi.shape[0], p=pi[:,s_t])\n",
        "                        s_t = s_t_next\n",
        "\n",
        "                Q[s,a] /= N\n",
        "\n",
        "        V_pi = np.einsum('sa,as->s', Q, pi)\n",
        "        psi = Q - np.outer(V_pi, np.ones(self.n_actions, dtype=float))\n",
        "\n",
        "        return (psi, V_pi)\n",
        "\n",
        "    def estimate_advantage_generative(self, pi, N, T):\n",
        "        \"\"\"\n",
        "        :param N: number of Monte Carlo simulations to run per state-action pair\n",
        "        :param T: duration to for each Monte Carlo simulation\n",
        "        \"\"\"\n",
        "        # 1 x S\n",
        "        pi_sum = np.cumsum(pi, axis=0)\n",
        "        # S x (SA)\n",
        "        P_reshape = np.reshape(self.P, newshape=(self.P.shape[0], self.P.shape[1]*self.P.shape[2]))\n",
        "        P_reshape_sum = np.cumsum(P_reshape, axis=0)\n",
        "\n",
        "        # SA\n",
        "        q = np.zeros(self.n_states*self.n_actions, dtype=float)\n",
        "\n",
        "        for i in range(N):\n",
        "            s_arr = np.kron(np.arange(self.n_states), np.ones(self.n_actions, dtype=int))\n",
        "            a_arr = np.kron(np.ones(self.n_states, dtype=int), np.arange(self.n_actions))\n",
        "            for t in range(T):\n",
        "                q += self.gamma**t * self.c[s_arr, a_arr]\n",
        "\n",
        "                u = self.rng.uniform(size=len(q))\n",
        "                z_arr = s_arr * self.n_actions + a_arr\n",
        "                s_arr = np.argmax(np.outer(u, np.ones(self.n_states)) < P_reshape_sum[:,z_arr].T, axis=1)\n",
        "\n",
        "                u = self.rng.uniform(size=len(q))\n",
        "                a_arr = np.argmax(np.outer(u, np.ones(self.n_actions)) < pi_sum[:,s_arr].T, axis=1)\n",
        "\n",
        "        q /= N\n",
        "        Q = np.reshape(q, newshape=(self.n_states, self.n_actions))\n",
        "\n",
        "        V_pi = np.einsum('sa,as->s', Q, pi)\n",
        "        psi = Q - np.outer(V_pi, np.ones(self.n_actions, dtype=float))\n",
        "\n",
        "        return (psi, V_pi)\n",
        "\n",
        "    def estimate_advantage_online_mc(self, pi, T, threshold=0, bootstrap=False):\n",
        "        \"\"\"\n",
        "        https://arxiv.org/pdf/2303.04386\n",
        "\n",
        "        :param T: duration to run Monte Carlo simulation\n",
        "        :param threshold: pi(a|s) < threshold means Q(s,a)=largest value, do not visit again (rec: (1-gamma)**2/|A|)\n",
        "        :return visit_len_state_action: how long the Monte carlo estimate is at every state-aciton pair\n",
        "        \"\"\"\n",
        "        costs = np.zeros(T, dtype=float)\n",
        "        states = np.zeros(T, dtype=int)\n",
        "        actions = np.zeros(T, dtype=int)\n",
        "\n",
        "        for t in range(T):\n",
        "            states[t] = self.s\n",
        "            actions[t] = self.rng.choice(pi.shape[0], p=pi[:,states[t]])\n",
        "            costs[t] = self.c[states[t], actions[t]]\n",
        "            self.s = self.rng.choice(self.P.shape[0], p=self.P[:,states[t],actions[t]])\n",
        "\n",
        "        # check bootstrap\n",
        "        if bootstrap and self.init_linear:\n",
        "            a_t = self.rng.choice(pi.shape[0], p=pi[:,self.s])\n",
        "            costs[-1] += self.gamma * self.predict([[self.s,a_t]])\n",
        "\n",
        "        # form advantage (dp style);\n",
        "        cumulative_discounted_costs = np.zeros(T, dtype=float)\n",
        "        cumulative_discounted_costs[-1] = costs[-1]\n",
        "        for t in range(T-2,-1,-1):\n",
        "            cumulative_discounted_costs[t] = costs[t] + self.gamma*cumulative_discounted_costs[t+1]\n",
        "\n",
        "        Q = np.zeros((self.n_states, self.n_actions), dtype=float)\n",
        "        visit_len_state_action = np.zeros((self.n_states, self.n_actions), dtype=bool)\n",
        "        for t in range(T):\n",
        "            (s,a) = states[t], actions[t]\n",
        "            if visit_len_state_action[s,a] > 0:\n",
        "                continue\n",
        "            Q[s,a] = cumulative_discounted_costs[t]\n",
        "            visit_len_state_action[s,a] = T-t\n",
        "\n",
        "        # for proabibilities that are very low, set Q value to be high\n",
        "        (poor_sa_a, poor_sa_s) = np.where(pi <= threshold)\n",
        "        Q_max = np.max(np.abs(self.c))/(1.-self.gamma)\n",
        "        Q[poor_sa_s,poor_sa_a] = Q_max\n",
        "\n",
        "        V_pi = np.einsum('sa,as->s', Q, pi)\n",
        "        psi = Q - np.outer(V_pi, np.ones(self.n_actions, dtype=float))\n",
        "\n",
        "        return (psi, V_pi, visit_len_state_action)\n",
        "\n",
        "    def init_estimate_advantage_online_linear(self, linear_settings):\n",
        "        \"\"\"\n",
        "        Prepares radial basis functions for linear function approximation:\n",
        "\n",
        "            https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html\n",
        "\n",
        "        See also: https://github.com/dennybritz/reinforcement-learning/blob/master/FA/Q-Learning%20with%20Value%20Function%20Approximation%20Solution.ipynb\n",
        "\n",
        "        :param X: Nxn array of inputs, where N is the number of datapoints and n is the size of the state space\n",
        "\t    \"\"\"\n",
        "\n",
        "        self.featurizer = sklearn.pipeline.FeatureUnion([\n",
        "            # (\"rbf0\", RBFSampler(gamma=5.0, n_components=100)),\n",
        "            (\"rbf1\", sklearn.kernel_approximation.RBFSampler(gamma=1.0, n_components=100)),\n",
        "            # (\"rbf2\", RBFSampler(gamma=0.1, n_components=100)),\n",
        "        ])\n",
        "\n",
        "        X = np.vstack((\n",
        "            np.kron(np.arange(self.n_states), np.ones(self.n_actions)),\n",
        "            np.kron(np.ones(self.n_states), np.arange(self.n_actions)),\n",
        "        )).T\n",
        "\n",
        "        self.featurizer.fit(X)\n",
        "        self.model = sklearn.linear_model.SGDRegressor(\n",
        "            learning_rate=linear_settings[\"linear_learning_rate\"],\n",
        "            eta0=linear_settings[\"linear_eta0\"],\n",
        "            max_iter=linear_settings[\"linear_max_iter\"],\n",
        "            alpha=linear_settings[\"linear_alpha\"],\n",
        "            warm_start=True,\n",
        "            tol=0.0,\n",
        "            n_iter_no_change=linear_settings[\"linear_max_iter\"],\n",
        "            fit_intercept=True,\n",
        "        )\n",
        "\n",
        "        # We need to call partial_fit once to initialize the model or we get a\n",
        "        # NotFittedError when trying to make a prediction This is quite hacky.\n",
        "        self.model.partial_fit(self.featurize([X[0]]), [0])\n",
        "        self.init_linear = True\n",
        "\n",
        "    def featurize(self, X):\n",
        "        return self.featurizer.transform(X).astype('float64')\n",
        "\n",
        "    def predict(self, x):\n",
        "        features = self.featurize(x)\n",
        "        output = np.squeeze(self.model.predict(features))\n",
        "        return output\n",
        "\n",
        "    def get_all_sa_pairs_for_finite(self):\n",
        "        X_all_sa = np.vstack((\n",
        "            np.kron(np.arange(self.n_states), np.ones(self.n_actions)),\n",
        "            np.kron(np.ones(self.n_states), np.arange(self.n_actions)),\n",
        "        )).T\n",
        "        return X_all_sa\n",
        "\n",
        "    def custom_SGD(solver, X, y, minibatch=32):\n",
        "        n_epochs = solver.max_iter\n",
        "        n_consec_regress_epochs = 0\n",
        "        max_regress = solver.n_iter_no_change\n",
        "        frac_validation = solver.validation_fraction\n",
        "        tol = solver.tol\n",
        "        early_stopping = solver.early_stopping\n",
        "\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "\n",
        "        for i in range(n_epochs):\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=i, shuffle=True, test_size=frac_validation)\n",
        "            num_batches = int(np.ceil(len(X_train)/ minibatch))\n",
        "            for j in range(num_batches):\n",
        "                k_s = minibatch*j\n",
        "                k_e = min(len(X_train), minibatch*(j+1))\n",
        "                # mini-batch update\n",
        "                solver.partial_fit(X_train[k_s:k_e], y_train[k_s:k_e])\n",
        "\n",
        "            y_train_pred = solver.predict(X_train)\n",
        "            y_test_pred = solver.predict(X_test)\n",
        "\n",
        "            train_losses.append(la.norm(y_train_pred - y_train)**2/len(y_train))\n",
        "            test_losses.append(la.norm(y_test_pred - y_test)**2/len(y_test))\n",
        "\n",
        "            if early_stopping and len(test_losses) > 1 and test_losses[-1] > np.min(test_losses)-tol:\n",
        "                n_consec_regress_epochs += 1\n",
        "            else:\n",
        "                n_consec_regress_epochs = 0\n",
        "            if n_consec_regress_epochs == max_regress:\n",
        "                print(\"Early stopping (stagnate)\")\n",
        "                break\n",
        "            if train_losses[-1] <= tol:\n",
        "                print(\"Early stopping (train loss small)\")\n",
        "                break\n",
        "\n",
        "        return np.array(train_losses), np.array(test_losses)\n",
        "\n",
        "    # https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_early_stopping.html#sphx-glr-auto-examples-linear-model-plot-sgd-early-stopping-py\n",
        "    @ignore_warnings(category=ConvergenceWarning)\n",
        "    def estimate_advantage_online_linear(self, pi, T):\n",
        "        \"\"\"\n",
        "        Use Monte Carlo simulation to obtain partial Q function.  We use linear\n",
        "        function approximation with bootstrap to update sampled sa pairs and\n",
        "        fill in missing sa pairs.\n",
        "\n",
        "        :param T: duration to run Monte Carlo simulation\n",
        "        \"\"\"\n",
        "        assert self.init_linear, \"Run `init_estimate_advantage_online_linear` before estimating\"\n",
        "\n",
        "        # use monte carlo estimate to estimate truncated psi (threshold=0\n",
        "        # ensures non-visited sa have zero value, i.e., Q[s,a]=0)\n",
        "        output = self.estimate_advantage_online_mc(pi, T, threshold=0, bootstrap=True)\n",
        "        (psi, V_pi, visit_len_state_action) = output\n",
        "        Q = psi + np.outer(V_pi, np.ones(self.n_actions, dtype=float))\n",
        "\n",
        "        # bootstrap remaining cost-to-go values\n",
        "        # X_all_sa = self.get_all_sa_pairs_for_finite()\n",
        "        # y = self.predict(X_all_sa)\n",
        "\n",
        "        visited_sa_s, visited_sa_a = np.where(visit_len_state_action >= 1)\n",
        "        X_visited_sa = np.vstack((visited_sa_s, visited_sa_a)).T\n",
        "        # state-action pair index in 1D\n",
        "        visited_idxs = self.n_actions * visited_sa_s + visited_sa_a\n",
        "\n",
        "        # y = Q.flatten() + np.multiply(np.power(self.gamma, visit_len_state_action.flatten()), y)\n",
        "        # visited_idxs = np.where(visit_len_state_action.flatten() > 0)[0]\n",
        "        y = Q.flatten()[visited_idxs]\n",
        "        # for i, (s,a) in zip(visited_idxs, X_visited_sa):\n",
        "        #     y[i] = Q[s,a] + self.gamma**visit_len_state_action[s,a]*y[i]\n",
        "\n",
        "        # training update\n",
        "        # features = self.featurize(X_visited_sa)\n",
        "        # self.model.fit(features, y[visited_idxs])\n",
        "        # features = self.featurize(X_all_sa)\n",
        "        # self.model.fit(features, y)\n",
        "        features = self.featurize(X_visited_sa)\n",
        "        self.model.fit(features, y)\n",
        "\n",
        "        # predict psi_pi\n",
        "        X_all_sa = self.get_all_sa_pairs_for_finite()\n",
        "        q_pred = self.predict(X_all_sa)\n",
        "        Q_pred = np.reshape(q_pred, newshape=(self.n_states, self.n_actions))\n",
        "        V_pred = np.einsum('sa,as->s', Q_pred, pi)\n",
        "        psi_pred = Q_pred - np.outer(V_pred, np.ones(self.n_actions, dtype=float))\n",
        "\n",
        "        return (psi_pred, V_pred)\n",
        "\n",
        "    def get_steadystate(self, pi):\n",
        "        P_pi = np.einsum('psa,as->ps', self.P, pi)\n",
        "\n",
        "        dim = P_pi.shape[0]\n",
        "        Q = (P_pi.T-np.eye(dim))\n",
        "        ones = np.ones(dim)\n",
        "        Q = np.c_[Q,ones]\n",
        "        QTQ = np.dot(Q, Q.T)\n",
        "\n",
        "        # check singular\n",
        "        try:\n",
        "            if la.matrix_rank(QTQ) < QTQ.shape[0]:\n",
        "                print(\"Singular matrix when computing stationary distribution, return zero vector\")\n",
        "                return np.zeros(QTQ.shape[0], dtype=float)\n",
        "        except:\n",
        "            # error with matrix rank\n",
        "            return np.zeros(QTQ.shape[0], dtype=float)\n",
        "\n",
        "        bQT = np.ones(dim)\n",
        "        return np.linalg.solve(QTQ,bQT)\n",
        "\n",
        "class GridWorldWithTraps(MDPModel):\n",
        "\n",
        "    def __init__(self, length, n_traps, gamma, n_origins=-1, eps=0.05, seed=None, ergodic=False):\n",
        "        \"\"\" Creates 2D gridworld with side length @length grid world with traps.\n",
        "\n",
        "        Each step incurs a cost of +1\n",
        "        @n_traps traps are randomly placed. Stepping on it will incur a high an addition cost of +5\n",
        "        Reaching the target state will incur a cost of +0 and the agent will remain there.\n",
        "\n",
        "        If :ergodic:=True mode, then reaching the target incurs a -length cost\n",
        "        and the next state is a random non-target non-trap state. This ensures\n",
        "        all state-action spaces can be visited after reaching the target.\n",
        "\n",
        "        The agent can move in one of the four cardinal directions, if feasible.\n",
        "        There is a @eps probability another random direction will be selected.\n",
        "        \"\"\"\n",
        "\n",
        "        self.length = length\n",
        "        n_states = length*length\n",
        "        n_actions = 4\n",
        "        n_traps = min(n_traps, n_states-1)\n",
        "        if n_origins == -1:\n",
        "            n_origins = n_states-n_traps-1\n",
        "\n",
        "        # have the same set of traps, origins, and traps\n",
        "        rng = np.random.default_rng(seed)\n",
        "        rnd_pts = rng.choice(length*length, replace=False, size=n_traps+1+n_origins)\n",
        "\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.traps = traps = rnd_pts[:n_traps]\n",
        "        self.origins = origins = rnd_pts[n_traps:n_traps+n_origins]\n",
        "        rho = np.zeros(length*length, dtype=float)\n",
        "        rho[origins] = 1./len(origins)\n",
        "        self.target = target = rnd_pts[-1]\n",
        "        if len(origins) < 10:\n",
        "            print(\"  Origins at \", np.sort(origins))\n",
        "\n",
        "        P = np.zeros((n_states, n_states, n_actions), dtype=float)\n",
        "        c = np.zeros((n_states, n_actions), dtype=float)\n",
        "\n",
        "        def fill_gw_P_at_xy(P, x, y):\n",
        "            \"\"\"\n",
        "            Applies standard probability in the 4 cardinal directions provided by @x and @y\n",
        "\n",
        "            :param x: x-axis locations of source we want to move from\n",
        "            :param y: y-axis locations of source we want to move from\n",
        "            :param length: length of x and y-axis\n",
        "            :param eps: random probability of moving in another direction\n",
        "            \"\"\"\n",
        "            s = length*y+x\n",
        "            for a in range(4):\n",
        "                next_x = np.clip(x + DIRS[a][0], 0, length-1)\n",
        "                next_y = np.clip(y + DIRS[a][1], 0, length-1)\n",
        "                next_s = length*next_y+next_x\n",
        "                P[next_s, s, a] = (1.-eps)\n",
        "\n",
        "                # random action\n",
        "                for b in range(4):\n",
        "                    if b==a: continue\n",
        "                    next_x = np.clip(x + DIRS[b][0], 0, length-1)\n",
        "                    next_y = np.clip(y + DIRS[b][1], 0, length-1)\n",
        "                    next_s = length*next_y+next_x\n",
        "                    P[next_s, s, a] += eps/3 # add to not over-write\n",
        "\n",
        "        # handle corners\n",
        "        for i in range(4):\n",
        "            x = (length-1)*(i%2)\n",
        "            y = (length-1)*(i//2)\n",
        "            fill_gw_P_at_xy(P, x, y)\n",
        "\n",
        "        # vertical edges\n",
        "        for i in range(2):\n",
        "            x = (length-1)*i\n",
        "            y = np.arange(1,length-1)\n",
        "            fill_gw_P_at_xy(P, x, y)\n",
        "\n",
        "        # horizontal edges\n",
        "        for i in range(2):\n",
        "            y = (length-1)*i\n",
        "            x = np.arange(1,length-1)\n",
        "            fill_gw_P_at_xy(P, x, y)\n",
        "\n",
        "        # inner squares\n",
        "        x = np.kron(np.ones(length, dtype=int), np.arange(1, length-1))\n",
        "        y = np.kron(np.arange(1, length-1), np.ones(length, dtype=int))\n",
        "        fill_gw_P_at_xy(P, x, y)\n",
        "\n",
        "        # target\n",
        "        if ergodic:\n",
        "            # rnd_pts = rng.choice(length*length, replace=False, size=n_traps+1)\n",
        "            # non_target_nor_trap = np.setdiff1d(np.arange(length*length), rnd_pts)\n",
        "\n",
        "            P[:,target,:] = 0\n",
        "            # go to random non-target non-trap location\n",
        "            P[origins,target,:] = 1./len(origins)\n",
        "        else:\n",
        "            P[:,target,:] = 0\n",
        "            # stay at target\n",
        "            P[target,target,:] = 1.\n",
        "\n",
        "        # apply trap cost\n",
        "        c[:,:] = 1.\n",
        "        c[traps,:] = 10.\n",
        "        c[target,:] = -10.\n",
        "\n",
        "        super().__init__(n_states, n_actions, c, P, gamma, rho, seed)\n",
        "\n",
        "    def get_target(self):\n",
        "        return self.target\n",
        "\n",
        "    def init_agent(self):\n",
        "        self.agent = self.rng.choice(self.origins)\n",
        "        self.curr_time = 0\n",
        "        return self.agent\n",
        "\n",
        "    def step(self, action):\n",
        "        self.agent = self.rng.choice(self.P.shape[0], p=self.P[:,self.agent, action])\n",
        "        self.curr_time += 1\n",
        "\n",
        "        if self.agent == self.target:\n",
        "            print(\"Target reached in %d steps! Resetting\" % self.curr_time)\n",
        "            self.curr_time = 0\n",
        "            self.agent = self.rng.choice(self.origins)\n",
        "        elif self.agent in self.traps:\n",
        "            print(\"Target hit a trap\")\n",
        "        elif self.curr_time >= 50:\n",
        "            print(\"Agent stalled, resetting\")\n",
        "            self.curr_time = 0\n",
        "            self.agent = self.rng.choice(self.origins)\n",
        "\n",
        "        return self.agent\n",
        "\n",
        "    def print_grid(self):\n",
        "        # next_s = length*next_y+next_x\n",
        "        if not hasattr(self, \"grid_pt\"):\n",
        "            self.grid_pt = [ [' ']*self.length for _ in range(self.length) ]\n",
        "            # target\n",
        "            self.grid_pt[self.target//self.length][self.target % self.length] = 'D'\n",
        "            for trap in self.traps:\n",
        "                (y,x) = (trap // self.length, trap % self.length)\n",
        "                self.grid_pt[y][x] = 'T'\n",
        "\n",
        "        # agent\n",
        "        self.grid_pt[self.agent//self.length][self.agent% self.length] = '*'\n",
        "\n",
        "        msg = \"|\" + \"-\"*(self.length*2-1) + \"|\\n\"\n",
        "        for row in self.grid_pt:\n",
        "            msg += \"|\" + ':'.join(row) + \"|\\n\"\n",
        "        msg += \"|\" + \"-\"*(self.length*2-1) + \"|\\n\"\n",
        "        print(msg, end=\"\")\n",
        "\n",
        "        # agent\n",
        "        if self.agent not in self.traps:\n",
        "            self.grid_pt[self.agent//self.length][self.agent% self.length] = ' '\n",
        "        else:\n",
        "            self.grid_pt[self.agent//self.length][self.agent% self.length] = 'T'\n",
        "\n",
        "class Taxi(MDPModel):\n",
        "\n",
        "    # R, Y, G, B (x,y)\n",
        "    color_arr = [(0,0), (0,4), (4,0), (3,4)]\n",
        "\n",
        "    right_wall_arr = [(1,0), (1,1), (0,3), (0,4), (2,3), (2,4)]\n",
        "    left_wall_arr  = [(2,0), (2,1), (1,3), (1,4), (3,3), (3,4)]\n",
        "\n",
        "    def __init__(self, gamma, eps=0., n_origins=-1, ergodic=False, seed=None):\n",
        "        \"\"\" Creates 2D gridworld of fixed length=5 with a passenger at one of\n",
        "        the 4 locations that needs to be dropped off at one of the hotel locations.\n",
        "        The map appears as (see color_arr):\n",
        "\n",
        "            +---------+\n",
        "            |R: | : :G|\n",
        "            | : | : : |\n",
        "            | : : : : |\n",
        "            | | : | : |\n",
        "            |Y| : |B: |\n",
        "            +---------+\n",
        "\n",
        "        Based on: https://gymnasium.farama.org/environments/toy_text/taxi/\n",
        "\n",
        "        Each step incurs a cost of +1.\n",
        "        Correctly dropping off the passenger incurs a \"cost\" of -20.\n",
        "        Illegally picking up or dropping a passenger incurs a high cost of 10.\n",
        "\n",
        "        The agent can move in one of the four cardinal directions, if feasible.\n",
        "        There is a @eps probability another random direction will be selected.\n",
        "        In addition, there are two additional actions: pickup and drop off.\n",
        "        \"\"\"\n",
        "        length = 5\n",
        "\n",
        "        # 5 locations for passenger (pass_loc=4 means it is in taxi), and 4 destinations\n",
        "        n_states = length*length*5*4\n",
        "        n_actions = 6\n",
        "        if n_origins == -1:\n",
        "            n_origins = 5*5*4*3 # all possible places except when passenger is in taxi or destination\n",
        "\n",
        "        P = np.zeros((n_states, n_states, n_actions), dtype=float)\n",
        "        c = np.zeros((n_states, n_actions), dtype=float)\n",
        "\n",
        "        def fill_gw_P_at_xy(P, x, y, length, eps):\n",
        "            \"\"\"\n",
        "            Applies standard probability in the 4 cardinal directions provided by @x and @y\n",
        "\n",
        "            :param x: x-axis locations of source we want to move from\n",
        "            :param y: y-axis locations of source we want to move from\n",
        "            :param length: length of x and y-axis\n",
        "            :param eps: random probability of moving in another direction\n",
        "            \"\"\"\n",
        "            offsets = [p_loc*25+d_loc*125 for d_loc in range(4) for p_loc in range(5)]\n",
        "            s = length*y+x\n",
        "            for a in range(4):\n",
        "                next_x = np.clip(x + DIRS[a][0], 0, length-1)\n",
        "                next_y = np.clip(y + DIRS[a][1], 0, length-1)\n",
        "                for offset in offsets:\n",
        "                    curr_s = s + offset\n",
        "                    next_s = length*next_y+next_x+offset\n",
        "                    P[next_s, curr_s, a] = (1.-eps)\n",
        "\n",
        "                # random action\n",
        "                for b in range(4):\n",
        "                    if b==a: continue\n",
        "                    next_x = np.clip(x + DIRS[b][0], 0, length-1)\n",
        "                    next_y = np.clip(y + DIRS[b][1], 0, length-1)\n",
        "                    for offset in offsets:\n",
        "                        curr_s = s + offset\n",
        "                        next_s = length*next_y+next_x+offset\n",
        "                        P[next_s, curr_s, a] += eps/3 # add to not over-write\n",
        "\n",
        "        # handle corners\n",
        "        for i in range(4):\n",
        "            x = (length-1)*(i%2)\n",
        "            y = (length-1)*(i//2)\n",
        "            fill_gw_P_at_xy(P, x, y, length, eps)\n",
        "\n",
        "        # vertical edges\n",
        "        for i in range(2):\n",
        "            x = (length-1)*i\n",
        "            y = np.arange(1,length-1)\n",
        "            fill_gw_P_at_xy(P, x, y, length, eps)\n",
        "\n",
        "        # horizontal edges\n",
        "        for i in range(2):\n",
        "            y = (length-1)*i\n",
        "            x = np.arange(1,length-1)\n",
        "            fill_gw_P_at_xy(P, x, y, length, eps)\n",
        "\n",
        "        # inner squares\n",
        "        x = np.kron(np.ones(length, dtype=int), np.arange(1, length-1))\n",
        "        y = np.kron(np.arange(1, length-1), np.ones(length, dtype=int))\n",
        "        fill_gw_P_at_xy(P, x, y, length, eps)\n",
        "\n",
        "        # hit a wall\n",
        "        for right_wall in self.right_wall_arr:\n",
        "            loc_x, loc_y = right_wall\n",
        "            taxi_state = loc_x + 5*loc_y\n",
        "            offsets = [p_loc*25+d_loc*125 for d_loc in range(4) for p_loc in range(5)]\n",
        "            for offset in offsets:\n",
        "                curr_state = taxi_state + offset\n",
        "                # See DIRS\n",
        "                P[:, curr_state, 0] = 0\n",
        "                P[curr_state, curr_state, 0] = 1\n",
        "\n",
        "        for left_wall in self.left_wall_arr:\n",
        "            loc_x, loc_y = right_wall\n",
        "            taxi_state = loc_x + 5*loc_y\n",
        "            offsets = [p_loc*25+d_loc*125 for d_loc in range(4) for p_loc in range(5)]\n",
        "            for offset in offsets:\n",
        "                curr_state = taxi_state + offset\n",
        "                # See DIRS\n",
        "                P[:, curr_state, 2] = 0\n",
        "                P[curr_state, curr_state, 2] = 1\n",
        "\n",
        "        # apply step cost\n",
        "        c[:,:] = 1.\n",
        "\n",
        "        # (illegal) passenger pickup and drop off\n",
        "        all_state_arr = np.arange(5*5*5*4)\n",
        "        P[all_state_arr, all_state_arr, 4] = 1\n",
        "        P[all_state_arr, all_state_arr, 5] = 1\n",
        "        c[all_state_arr, 4] = 10\n",
        "        c[all_state_arr, 5] = 10\n",
        "\n",
        "        # legal passenger pickup\n",
        "        for i, (x,y) in enumerate(self.color_arr):\n",
        "            s = length*y+x\n",
        "            old_passenger_loc = 25*i\n",
        "            passenger_in_taxi_loc = 25*4\n",
        "            destination_loc_arr = 125*np.arange(4)\n",
        "\n",
        "            curr_state_arr = s + old_passenger_loc + destination_loc_arr\n",
        "            next_state_arr = s + passenger_in_taxi_loc + destination_loc_arr\n",
        "\n",
        "            P[:, curr_state_arr, 4] = 0\n",
        "            P[next_state_arr, curr_state_arr, 4] = 1\n",
        "            c[curr_state_arr, 4] = 1\n",
        "\n",
        "        # we can only start where passenger is neither in taxi nor destination\n",
        "        starting_states = np.array([], dtype=int)\n",
        "        for passenger_loc in range(4):\n",
        "            for destination_loc in range(4):\n",
        "                if passenger_loc == destination_loc:\n",
        "                    break\n",
        "                offset = passenger_loc*25 + destination_loc*125\n",
        "                starting_states = np.append(starting_states, np.arange(25)+offset)\n",
        "\n",
        "        rng = np.random.default_rng(0)\n",
        "        starting_states = rng.choice(starting_states, size=min(n_origins, len(starting_states)), replace=False)\n",
        "\n",
        "        # legal passenger dropoff\n",
        "        for i, (x,y) in enumerate(self.color_arr):\n",
        "            s = length*y+x\n",
        "            old_passenger_loc = 25*4\n",
        "            new_passenger_loc = 25*i\n",
        "            destination_loc = 125*i\n",
        "\n",
        "            curr_state_loc = s + old_passenger_loc + destination_loc\n",
        "            next_state_loc = s + new_passenger_loc + destination_loc\n",
        "\n",
        "            if ergodic:\n",
        "                P[:, curr_state_loc, 5] = 0\n",
        "                P[starting_states, curr_state_loc, 5] = 1./len(starting_states)\n",
        "            else:\n",
        "                P[:, curr_state_loc, 5] = 0\n",
        "                P[next_state_loc, curr_state_loc, 5] = 1\n",
        "                P[:, next_state_loc, :] = 0\n",
        "                P[next_state_loc, next_state_loc, :] = 1\n",
        "                c[next_state_arr, :] = 0\n",
        "\n",
        "            c[curr_state_loc, 5] = -20\n",
        "\n",
        "        super().__init__(n_states, n_actions, c, P, gamma, seed=seed)\n",
        "\n",
        "def get_env(name, gamma, seed=None):\n",
        "\n",
        "    if name == \"gridworld\":\n",
        "        env = GridWorldWithTraps(20, 50, gamma, seed=seed, ergodic=True)\n",
        "    elif name == \"taxi\":\n",
        "        env = Taxi(gamma, ergodic=True)\n",
        "    else:\n",
        "        raise Exception(\"Unknown env_name=%s\" % name)\n",
        "\n",
        "    return env\n"
      ],
      "metadata": {
        "id": "kFnsvbdKGh3H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def policy_update(pi, psi, eta, q=1.0):\n",
        "    \"\"\" Closed-form solution with PMD subproblem\n",
        "\n",
        "    :param pi (np.ndarray): current policy\n",
        "    :param psi (np.ndarray): current policy's advantage function\n",
        "    :param eta (float): step size\n",
        "    :param q (float): Tsallis entropic index (q = 1 > KL divergence)\n",
        "    :return (np.ndarray): next policy (should be same shape as @pi)\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply psi to update the policy\n",
        "    if q == 1.0: # KL\n",
        "        updated = pi * np.exp(-eta * psi.T)\n",
        "    else: # Tsallis\n",
        "        base = pi ** (1 - q) - (1 - q) * eta * psi.T\n",
        "        base = np.maximum(base, 0)\n",
        "        updated = base ** (1 / (1 - q))\n",
        "\n",
        "    # Normalize across actions\n",
        "    updated /= np.sum(updated, axis=0, keepdims=True)\n",
        "\n",
        "    return updated\n",
        "\n",
        "def simulate_agent(env, pi, T=100):\n",
        "    s = env.init_agent()\n",
        "    env.print_grid()\n",
        "    rng = np.random.default_rng()\n",
        "\n",
        "    for _ in range(T):\n",
        "        a = rng.choice(pi.shape[0], p=pi[:,s])\n",
        "        s = env.step(a)\n",
        "        time.sleep(0.5)\n",
        "        #env.print_grid()\n",
        "\n",
        "def train(settings, env = None):\n",
        "    if env is None:\n",
        "      env = get_env(settings['env_name'], settings['gamma'], settings['seed'])\n",
        "\n",
        "    # print formatter\n",
        "    exp_metadata = [\"Iter\", \"Est f(pi)\", \"Est f(pi*)\", \"Est gap\"]\n",
        "    row_format =\"{:>5}|{:>10}|{:>10}|{:>10}\"\n",
        "    print(\"\")\n",
        "    print(row_format.format(*exp_metadata))\n",
        "    print(\"-\" * (35+len(exp_metadata)-1))\n",
        "\n",
        "    # initial policy\n",
        "    pi_t = np.ones((env.n_actions, env.n_states), dtype=float)/env.n_actions\n",
        "\n",
        "    agg_psi_t = np.zeros((env.n_states, env.n_actions), dtype=float)\n",
        "    agg_V_t = np.zeros(env.n_states, dtype=float)\n",
        "\n",
        "    if settings['advantage'] == 'linear':\n",
        "        env.init_estimate_advantage_online_linear({\n",
        "            \"linear_learning_rate\": \"constant\",\n",
        "            \"linear_eta0\": 0.1,\n",
        "            \"linear_max_iter\": 300,\n",
        "            \"linear_alpha\": 0.0001\n",
        "        })\n",
        "\n",
        "    s_time = time.time()\n",
        "    for t in range(settings[\"n_iters\"]):\n",
        "        if settings['advantage'] == 'generative':\n",
        "            (psi_t, V_t) = env.estimate_advantage_generative(pi_t, settings[\"N\"], settings[\"T\"]) # Generative\n",
        "        elif settings['advantage'] == 'linear':\n",
        "            (psi_t, V_t) = env.estimate_advantage_online_linear(pi_t, settings[\"T\"]) # Online Linear\n",
        "        elif settings['advantage'] == 'mc':\n",
        "            (psi_t, V_t, visit_len_state_action) = env.estimate_advantage_online_mc(pi_t, settings[\"T\"]*100, 0*((1 - env.gamma) ** 2) / env.n_actions, True) # Online MC\n",
        "        else:\n",
        "            print(\"ERROR: INCORRECT ADVANTAGE FUNCTION\")\n",
        "        adv_gap = np.max(-agg_psi_t, axis=1)/(1.-env.gamma)\n",
        "\n",
        "        # NOT STEP SIZE, JUST AN ARTIFACT TO DO THE WEIGHTED SU<\n",
        "        alpha_t = 1./(t+1)\n",
        "        agg_psi_t = (1.-alpha_t)*agg_psi_t + alpha_t*psi_t\n",
        "        agg_V_t = (1.-alpha_t)*agg_V_t + alpha_t*V_t\n",
        "\n",
        "        if ((t+1) <= 100 and (t+1) % 5 == 0) or (t+1) % 25==0:\n",
        "            print(row_format.format(\n",
        "                t+1,\n",
        "                \"%.2e\" % np.dot(env.rho, V_t),\n",
        "                \"%.2e\" % np.dot(env.rho, agg_V_t - adv_gap),\n",
        "                \"%.2e\" % (np.dot(env.rho, V_t) - np.dot(env.rho, agg_V_t - adv_gap)),\n",
        "            ))\n",
        "\n",
        "        # eta_t = settings[\"alpha\"]/(t+1)**0.5\n",
        "        # THIS IS STEP SIZE\n",
        "        eta_t = settings[\"alpha\"]/(settings[\"n_iters\"])**0.5\n",
        "        pi_t = policy_update(pi_t, psi_t, eta_t, settings['divergence_shape'])\n",
        "\n",
        "    print(\"Total runtime: %.2fs\" % (time.time() - s_time))\n",
        "\n",
        "    (true_psi_t, true_V_t) = env.get_advantage(pi_t)\n",
        "    adv_gap = np.max(-true_psi_t, axis=1)/(1.-env.gamma)\n",
        "\n",
        "    print(\"=== Final performance metric ===\")\n",
        "    print(\"  f(pi_k):   %.4e\\n  f(pi*) lb: %.4e\\n  Gap:       %.4e\" % (\n",
        "        np.dot(env.rho, true_V_t),\n",
        "        np.dot(env.rho, true_V_t - adv_gap),\n",
        "        np.dot(env.rho, adv_gap),\n",
        "    ))\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    if settings[\"visual\"] and settings['env_name'] == 'gridworld':\n",
        "        simulate_agent(env, pi_t)\n",
        "\n",
        "    return pi_t\n",
        "\n"
      ],
      "metadata": {
        "id": "nBA27Yp4Gpkz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings = dict({\n",
        "      \"alpha\": 1,\n",
        "      \"visual\": \"store_true\",\n",
        "      \"N\": 1,\n",
        "      \"T\": 50,\n",
        "      \"gamma\": 0.9,\n",
        "      \"env_name\": 'gridworld',\n",
        "      \"n_iters\": 200,\n",
        "      \"seed\": 0, # fixed seed so that i get the same env each time\n",
        "      \"advantage\": \"generative\",\n",
        "      \"divergence_shape\": 1.0\n",
        "  })\n",
        "env = get_env(settings['env_name'], settings['gamma'], settings['seed'])\n",
        "train(settings, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p6DjzdKGp43",
        "outputId": "c5ce275b-a051-4cd6-87bf-654735f663eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Iter| Est f(pi)|Est f(pi*)|   Est gap\n",
            "--------------------------------------\n",
            "    5|  1.06e+01| -1.22e+01|  2.27e+01\n",
            "   10|  9.15e+00| -1.93e+00|  1.11e+01\n",
            "   15|  8.43e+00|  1.19e+00|  7.24e+00\n",
            "   20|  8.22e+00|  2.50e+00|  5.72e+00\n",
            "   25|  7.41e+00|  3.11e+00|  4.30e+00\n",
            "   30|  7.23e+00|  3.54e+00|  3.69e+00\n",
            "   35|  6.98e+00|  3.79e+00|  3.19e+00\n",
            "   40|  6.60e+00|  4.04e+00|  2.55e+00\n",
            "   45|  6.50e+00|  4.19e+00|  2.31e+00\n",
            "   50|  6.22e+00|  4.29e+00|  1.93e+00\n",
            "   55|  6.28e+00|  4.36e+00|  1.92e+00\n",
            "   60|  6.08e+00|  4.44e+00|  1.64e+00\n",
            "   65|  5.96e+00|  4.51e+00|  1.45e+00\n",
            "   70|  5.96e+00|  4.56e+00|  1.40e+00\n",
            "   75|  5.82e+00|  4.60e+00|  1.22e+00\n",
            "   80|  5.84e+00|  4.61e+00|  1.23e+00\n",
            "   85|  5.64e+00|  4.65e+00|  9.94e-01\n",
            "   90|  5.61e+00|  4.67e+00|  9.41e-01\n",
            "   95|  5.77e+00|  4.69e+00|  1.07e+00\n",
            "  100|  5.76e+00|  4.70e+00|  1.06e+00\n",
            "  125|  5.50e+00|  4.78e+00|  7.20e-01\n",
            "  150|  5.49e+00|  4.86e+00|  6.33e-01\n",
            "  175|  5.46e+00|  4.90e+00|  5.64e-01\n",
            "  200|  5.60e+00|  4.95e+00|  6.53e-01\n",
            "Total runtime: 86.11s\n",
            "=== Final performance metric ===\n",
            "  f(pi_k):   5.4651e+00\n",
            "  f(pi*) lb: 5.2287e+00\n",
            "  Gap:       2.3642e-01\n",
            "========================================\n",
            "|---------------------------------------|\n",
            "| : :T: : : : : : :T:T: : : : : : :T: : |\n",
            "| : : : : : : : : : : : : : : : :T:T: :T|\n",
            "| :T:T: : : : : : : : : :T: :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : : :T:T:T|\n",
            "|T: : : : : : :T: : : : : :T: : : : : : |\n",
            "| : :T: : : : :T: :T: : : : : : : : :T: |\n",
            "| : : :T:T: : : : : : : :T:T: : : : : : |\n",
            "| :T: : : : : :T: : : : : : : : : : : : |\n",
            "| : : :T: :T: : : : : : :T:T: : :T: :T: |\n",
            "| : : : :T: : : : : : : : : : :*: : : : |\n",
            "| : : : :T: : : : :T: : : : : : :T: : :T|\n",
            "| : : : : : : : : : : : : : : :D: : : :T|\n",
            "| : : : :T: : : : : : : : : : : : : : : |\n",
            "| : : :T: : : : : : : : : : :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : :T: : : |\n",
            "| : : : : :T: : : : : : : : : : : : : : |\n",
            "| : : : : : : : : : : : : : : : :T: : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "| : :T: : : : :T: : : : :T: : : : : : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "|---------------------------------------|\n",
            "Target reached in 2 steps! Resetting\n",
            "Target reached in 11 steps! Resetting\n",
            "Target reached in 12 steps! Resetting\n",
            "Target reached in 13 steps! Resetting\n",
            "Target reached in 13 steps! Resetting\n",
            "Target reached in 12 steps! Resetting\n",
            "Target reached in 13 steps! Resetting\n",
            "Target reached in 13 steps! Resetting\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.51824149e-01, 9.85670576e-45, 9.99528136e-01, ...,\n",
              "        1.25591795e-04, 2.35458762e-04, 1.31741306e-02],\n",
              "       [1.07351532e-01, 5.23722310e-01, 4.12259017e-04, ...,\n",
              "        4.67529294e-04, 1.68447900e-02, 2.87418388e-03],\n",
              "       [1.23483939e-01, 4.75843478e-01, 5.96046788e-05, ...,\n",
              "        3.57213680e-01, 4.20839050e-01, 8.04941174e-02],\n",
              "       [6.17340379e-01, 4.34211304e-04, 4.12491559e-48, ...,\n",
              "        6.42193199e-01, 5.62080701e-01, 9.03457568e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "settings = dict({\n",
        "      \"alpha\": 1,\n",
        "      \"visual\": \"store_true\",\n",
        "      \"N\": 1,\n",
        "      \"T\": 100,\n",
        "      \"gamma\": 0.9,\n",
        "      \"env_name\": 'gridworld',\n",
        "      \"n_iters\": 200,\n",
        "      \"seed\": 0, # fixed seed so that i get the same env each time\n",
        "      \"advantage\": \"linear\",\n",
        "      \"divergence_shape\": 1.0\n",
        "  })\n",
        "pi_t = train(settings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLYIel8_IGR3",
        "outputId": "3272ed1f-a796-43d7-fc37-14ca04cf5b4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Iter| Est f(pi)|Est f(pi*)|   Est gap\n",
            "--------------------------------------\n",
            "    5|  1.65e+01| -3.81e+01|  5.45e+01\n",
            "   10|  1.90e+01| -1.39e+01|  3.29e+01\n",
            "   15|  1.09e+01| -5.03e+00|  1.59e+01\n",
            "   20|  9.09e+00| -2.04e-01|  9.29e+00\n",
            "   25|  2.92e+01|  3.75e+00|  2.55e+01\n",
            "   30|  2.70e+01|  7.56e+00|  1.94e+01\n",
            "   35|  2.00e+01|  9.36e+00|  1.07e+01\n",
            "   40|  7.51e+00|  9.29e+00| -1.78e+00\n",
            "   45|  1.04e+01|  9.34e+00|  1.06e+00\n",
            "   50|  9.79e+00|  9.36e+00|  4.25e-01\n",
            "   55|  1.60e+01|  9.53e+00|  6.44e+00\n",
            "   60|  1.23e+01|  9.66e+00|  2.65e+00\n",
            "   65|  1.12e+01|  9.69e+00|  1.50e+00\n",
            "   70|  1.09e+01|  9.89e+00|  1.03e+00\n",
            "   75|  1.10e+01|  9.94e+00|  1.01e+00\n",
            "   80|  1.05e+01|  9.98e+00|  5.32e-01\n",
            "   85|  1.12e+01|  1.00e+01|  1.18e+00\n",
            "   90|  1.07e+01|  1.01e+01|  6.78e-01\n",
            "   95|  1.02e+01|  1.01e+01|  1.32e-01\n",
            "  100|  1.05e+01|  1.01e+01|  4.01e-01\n",
            "  125|  1.00e+01|  1.03e+01| -2.08e-01\n",
            "  150|  1.10e+01|  1.03e+01|  7.04e-01\n",
            "  175|  1.00e+01|  1.04e+01| -3.54e-01\n",
            "  200|  1.21e+01|  1.04e+01|  1.71e+00\n",
            "Total runtime: 3.10s\n",
            "=== Final performance metric ===\n",
            "  f(pi_k):   1.6632e+01\n",
            "  f(pi*) lb: -1.8370e+01\n",
            "  Gap:       3.5002e+01\n",
            "========================================\n",
            "|---------------------------------------|\n",
            "| : :T: : : : : : :T:T: : : : : : :T: : |\n",
            "| : : : : : : : : : : : : : : : :T:T: :T|\n",
            "| :T:T: : : : : : : : : :T: :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : : :T:T:T|\n",
            "|T: : : : : : :T: : : : : :T: : : : : : |\n",
            "| : :T: : : : :T: :T: : : : : : : : :T: |\n",
            "| : : :T:T: : : : : : : :T:T: : : : : : |\n",
            "| :T: : : : : :T: : : : : : : : : : : : |\n",
            "| : : :T: :T: : : : : : :T:T: : :T: :T: |\n",
            "| : : : :T: : : : : : : : : : :*: : : : |\n",
            "| : : : :T: : : : :T: : : : : : :T: : :T|\n",
            "| : : : : : : : : : : : : : : :D: : : :T|\n",
            "| : : : :T: : : : : : : : : : : : : : : |\n",
            "| : : :T: : : : : : : : : : :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : :T: : : |\n",
            "| : : : : :T: : : : : : : : : : : : : : |\n",
            "| : : : : : : : : : : : : : : : :T: : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "| : :T: : : : :T: : : : :T: : : : : : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "|---------------------------------------|\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n",
            "Target hit a trap\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n1SK8LKXFfp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine online learning with initial generative model"
      ],
      "metadata": {
        "id": "vflr-dBFFf5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def online_train(settings, env = None, pi0 = None):\n",
        "    if env is None:\n",
        "      env = get_env(settings['env_name'], settings['gamma'], settings['seed'])\n",
        "\n",
        "    # print formatter\n",
        "    exp_metadata = [\"Iter\", \"Est f(pi)\", \"Est f(pi*)\", \"Est gap\"]\n",
        "    row_format =\"{:>5}|{:>10}|{:>10}|{:>10}\"\n",
        "    print(\"\")\n",
        "    print(row_format.format(*exp_metadata))\n",
        "    print(\"-\" * (35+len(exp_metadata)-1))\n",
        "\n",
        "    # initial policy\n",
        "    pi_t = np.ones((env.n_actions, env.n_states), dtype=float)/env.n_actions\n",
        "\n",
        "    if pi0 is not None:\n",
        "        pi_t = pi0\n",
        "\n",
        "    agg_psi_t = np.zeros((env.n_states, env.n_actions), dtype=float)\n",
        "    agg_V_t = np.zeros(env.n_states, dtype=float)\n",
        "\n",
        "    if settings['advantage'] == 'linear':\n",
        "        env.init_estimate_advantage_online_linear({\n",
        "            \"linear_learning_rate\": \"constant\",\n",
        "            \"linear_eta0\": 0.1,\n",
        "            \"linear_max_iter\": 300,\n",
        "            \"linear_alpha\": 0.0001\n",
        "        })\n",
        "\n",
        "    s_time = time.time()\n",
        "    for t in range(settings[\"n_iters\"]):\n",
        "        if settings['advantage'] == 'generative':\n",
        "            (psi_t, V_t) = env.estimate_advantage_generative(pi_t, settings[\"N\"], settings[\"T\"]) # Generative\n",
        "        elif settings['advantage'] == 'linear':\n",
        "            (psi_t, V_t) = env.estimate_advantage_online_linear(pi_t, settings[\"T\"]) # Online Linear\n",
        "        elif settings['advantage'] == 'mc':\n",
        "            (psi_t, V_t, visit_len_state_action) = env.estimate_advantage_online_mc(pi_t, settings[\"T\"]*100, 0*((1 - env.gamma) ** 2) / env.n_actions, True) # Online MC\n",
        "        else:\n",
        "            print(\"ERROR: INCORRECT ADVANTAGE FUNCTION\")\n",
        "        adv_gap = np.max(-agg_psi_t, axis=1)/(1.-env.gamma)\n",
        "\n",
        "        # NOT STEP SIZE, JUST AN ARTIFACT TO DO THE WEIGHTED SU<\n",
        "        alpha_t = 1./(t+1)\n",
        "        agg_psi_t = (1.-alpha_t)*agg_psi_t + alpha_t*psi_t\n",
        "        agg_V_t = (1.-alpha_t)*agg_V_t + alpha_t*V_t\n",
        "\n",
        "        if ((t+1) <= 100 and (t+1) % 5 == 0) or (t+1) % 25==0:\n",
        "            print(row_format.format(\n",
        "                t+1,\n",
        "                \"%.2e\" % np.dot(env.rho, V_t),\n",
        "                \"%.2e\" % np.dot(env.rho, agg_V_t - adv_gap),\n",
        "                \"%.2e\" % (np.dot(env.rho, V_t) - np.dot(env.rho, agg_V_t - adv_gap)),\n",
        "            ))\n",
        "\n",
        "        # eta_t = settings[\"alpha\"]/(t+1)**0.5\n",
        "        # THIS IS STEP SIZE\n",
        "        eta_t = settings[\"alpha\"]/(settings[\"n_iters\"])**0.5\n",
        "        pi_t = policy_update(pi_t, psi_t, eta_t, settings['divergence_shape'])\n",
        "\n",
        "    print(\"Total runtime: %.2fs\" % (time.time() - s_time))\n",
        "\n",
        "    (true_psi_t, true_V_t) = env.get_advantage(pi_t)\n",
        "    adv_gap = np.max(-true_psi_t, axis=1)/(1.-env.gamma)\n",
        "\n",
        "    print(\"=== Final performance metric ===\")\n",
        "    print(\"  f(pi_k):   %.4e\\n  f(pi*) lb: %.4e\\n  Gap:       %.4e\" % (\n",
        "        np.dot(env.rho, true_V_t),\n",
        "        np.dot(env.rho, true_V_t - adv_gap),\n",
        "        np.dot(env.rho, adv_gap),\n",
        "    ))\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    if settings[\"visual\"] and settings['env_name'] == 'gridworld':\n",
        "        simulate_agent(env, pi_t)\n",
        "\n",
        "    return pi_t\n"
      ],
      "metadata": {
        "id": "IOIwIzsMLXdu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings = dict({\n",
        "      \"alpha\": 1,\n",
        "      \"visual\": \"store_true\",\n",
        "      \"N\": 1,\n",
        "      \"T\": 50,\n",
        "      \"gamma\": 0.9,\n",
        "      \"env_name\": 'gridworld',\n",
        "      \"n_iters\": 100,\n",
        "      \"seed\": 0, # fixed seed so that i get the same env each time\n",
        "      \"advantage\": \"generative\",\n",
        "      \"divergence_shape\": 1.0\n",
        "  })\n",
        "env = get_env(settings['env_name'], settings['gamma'], settings['seed'])\n",
        "pi100 = train(settings, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frmOTen3HPKW",
        "outputId": "6ca6e8ae-9970-424e-a317-354b63430007"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Iter| Est f(pi)|Est f(pi*)|   Est gap\n",
            "--------------------------------------\n",
            "    5|  9.97e+00| -1.09e+01|  2.09e+01\n",
            "   10|  8.95e+00| -9.64e-01|  9.91e+00\n",
            "   15|  8.20e+00|  1.70e+00|  6.50e+00\n",
            "   20|  7.73e+00|  2.87e+00|  4.86e+00\n",
            "   25|  7.17e+00|  3.46e+00|  3.71e+00\n",
            "   30|  6.99e+00|  3.81e+00|  3.18e+00\n",
            "   35|  6.72e+00|  4.05e+00|  2.67e+00\n",
            "   40|  6.28e+00|  4.24e+00|  2.04e+00\n",
            "   45|  6.13e+00|  4.37e+00|  1.77e+00\n",
            "   50|  5.93e+00|  4.45e+00|  1.48e+00\n",
            "   55|  6.08e+00|  4.51e+00|  1.57e+00\n",
            "   60|  5.74e+00|  4.56e+00|  1.19e+00\n",
            "   65|  5.78e+00|  4.58e+00|  1.20e+00\n",
            "   70|  5.81e+00|  4.62e+00|  1.18e+00\n",
            "   75|  5.63e+00|  4.66e+00|  9.70e-01\n",
            "   80|  5.73e+00|  4.67e+00|  1.06e+00\n",
            "   85|  5.59e+00|  4.71e+00|  8.83e-01\n",
            "   90|  5.58e+00|  4.71e+00|  8.70e-01\n",
            "   95|  5.72e+00|  4.73e+00|  9.86e-01\n",
            "  100|  5.66e+00|  4.76e+00|  9.04e-01\n",
            "Total runtime: 44.64s\n",
            "=== Final performance metric ===\n",
            "  f(pi_k):   5.5493e+00\n",
            "  f(pi*) lb: 5.1450e+00\n",
            "  Gap:       4.0435e-01\n",
            "========================================\n",
            "|---------------------------------------|\n",
            "| : :T: : : : : : :T:T: : : : : : :T: : |\n",
            "| : : : : : : : : : : : : : : : :T:T: :T|\n",
            "| :T:T: : : : : : : : : :T: :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : : :T:T:T|\n",
            "|T: : : : : : :T: : : : : :T: : : : : : |\n",
            "| : :T: : : : :T: :T: : : : : : : : :T: |\n",
            "| : : :T:T: : : : : : : :T:T: : : : : : |\n",
            "| :T: : : : : :T: : : : : : : : : : : : |\n",
            "| : : :T: :T: : : : : : :T:T: : :T: :T: |\n",
            "| : : : :T: : : : : : : : : : :*: : : : |\n",
            "| : : : :T: : : : :T: : : : : : :T: : :T|\n",
            "| : : : : : : : : : : : : : : :D: : : :T|\n",
            "| : : : :T: : : : : : : : : : : : : : : |\n",
            "| : : :T: : : : : : : : : : :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : :T: : : |\n",
            "| : : : : :T: : : : : : : : : : : : : : |\n",
            "| : : : : : : : : : : : : : : : :T: : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "| : :T: : : : :T: : : : :T: : : : : : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "|---------------------------------------|\n",
            "Target reached in 2 steps! Resetting\n",
            "Target reached in 10 steps! Resetting\n",
            "Target reached in 14 steps! Resetting\n",
            "Target reached in 8 steps! Resetting\n",
            "Target reached in 5 steps! Resetting\n",
            "Target hit a trap\n",
            "Target reached in 24 steps! Resetting\n",
            "Target hit a trap\n",
            "Target reached in 22 steps! Resetting\n",
            "Target reached in 4 steps! Resetting\n",
            "Target reached in 2 steps! Resetting\n",
            "Target reached in 4 steps! Resetting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens if we continue generative training"
      ],
      "metadata": {
        "id": "JLidl0tDMaxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "settings = dict({\n",
        "      \"alpha\": 1,\n",
        "      \"visual\": \"store_true\",\n",
        "      \"N\": 1,\n",
        "      \"T\": 50,\n",
        "      \"gamma\": 0.9,\n",
        "      \"env_name\": 'gridworld',\n",
        "      \"n_iters\": 100,\n",
        "      \"seed\": 0, # fixed seed so that i get the same env each time\n",
        "      \"advantage\": \"generative\",\n",
        "      \"divergence_shape\": 1.0\n",
        "  })\n",
        "env = get_env(settings['env_name'], settings['gamma'], settings['seed'])\n",
        "pi200 = online_train(settings, env, pi100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnD5cdNVMY3_",
        "outputId": "8326ebc8-64f3-43f9-f610-bb1082373f96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Iter| Est f(pi)|Est f(pi*)|   Est gap\n",
            "--------------------------------------\n",
            "    5|  5.43e+00|  1.40e+00|  4.04e+00\n",
            "   10|  5.64e+00|  2.74e+00|  2.90e+00\n",
            "   15|  5.38e+00|  3.40e+00|  1.98e+00\n",
            "   20|  5.49e+00|  3.79e+00|  1.70e+00\n",
            "   25|  5.38e+00|  3.99e+00|  1.38e+00\n",
            "   30|  5.64e+00|  4.26e+00|  1.39e+00\n",
            "   35|  5.57e+00|  4.38e+00|  1.19e+00\n",
            "   40|  5.35e+00|  4.47e+00|  8.81e-01\n",
            "   45|  5.39e+00|  4.55e+00|  8.39e-01\n",
            "   50|  5.26e+00|  4.64e+00|  6.16e-01\n",
            "   55|  5.58e+00|  4.70e+00|  8.76e-01\n",
            "   60|  5.37e+00|  4.76e+00|  6.17e-01\n",
            "   65|  5.48e+00|  4.80e+00|  6.84e-01\n",
            "   70|  5.49e+00|  4.82e+00|  6.71e-01\n",
            "   75|  5.32e+00|  4.83e+00|  4.82e-01\n",
            "   80|  5.56e+00|  4.87e+00|  6.89e-01\n",
            "   85|  5.40e+00|  4.89e+00|  5.09e-01\n",
            "   90|  5.43e+00|  4.93e+00|  5.02e-01\n",
            "   95|  5.63e+00|  4.96e+00|  6.66e-01\n",
            "  100|  5.57e+00|  4.98e+00|  5.91e-01\n",
            "Total runtime: 34.22s\n",
            "=== Final performance metric ===\n",
            "  f(pi_k):   5.4729e+00\n",
            "  f(pi*) lb: 5.2299e+00\n",
            "  Gap:       2.4302e-01\n",
            "========================================\n",
            "|---------------------------------------|\n",
            "| : :T: : : : : : :T:T: : : : : : :T: : |\n",
            "| : : : : : : : : : : : : : : : :T:T: :T|\n",
            "| :T:T: : : : : : : : : :T: :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : : :T:T:T|\n",
            "|T: : : : : : :T: : : : : :T: : : : : : |\n",
            "| : :T: : : : :T: :T: : : : : : : : :T: |\n",
            "| : : :T:T: : : : : : : :T:T: : : : : : |\n",
            "| :T: : : : : :T: : : : : : : : : : : : |\n",
            "| : : :T: :T: : : : : : :T:T: : :T: :T: |\n",
            "| : : : :T: : : : : : : : : : :*: : : : |\n",
            "| : : : :T: : : : :T: : : : : : :T: : :T|\n",
            "| : : : : : : : : : : : : : : :D: : : :T|\n",
            "| : : : :T: : : : : : : : : : : : : : : |\n",
            "| : : :T: : : : : : : : : : :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : :T: : : |\n",
            "| : : : : :T: : : : : : : : : : : : : : |\n",
            "| : : : : : : : : : : : : : : : :T: : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "| : :T: : : : :T: : : : :T: : : : : : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "|---------------------------------------|\n",
            "Target reached in 2 steps! Resetting\n",
            "Target reached in 10 steps! Resetting\n",
            "Target reached in 14 steps! Resetting\n",
            "Target reached in 8 steps! Resetting\n",
            "Target reached in 5 steps! Resetting\n",
            "Target reached in 22 steps! Resetting\n",
            "Target reached in 22 steps! Resetting\n",
            "Target reached in 2 steps! Resetting\n",
            "Target reached in 6 steps! Resetting\n",
            "Target reached in 4 steps! Resetting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with linear function approximation"
      ],
      "metadata": {
        "id": "Xg2tUhshMp1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "settings = dict({\n",
        "      \"alpha\": 1,\n",
        "      \"visual\": \"store_true\",\n",
        "      \"N\": 1,\n",
        "      \"T\": 50,\n",
        "      \"gamma\": 0.9,\n",
        "      \"env_name\": 'gridworld',\n",
        "      \"n_iters\": 100,\n",
        "      \"seed\": 0, # fixed seed so that i get the same env each time\n",
        "      \"advantage\": \"linear\",\n",
        "      \"divergence_shape\": 1.0\n",
        "  })\n",
        "pi_t = online_train(settings, env, pi100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naJx5yOkHRqR",
        "outputId": "f60fc327-f1c6-4d74-d94d-d7b8e386f1fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Iter| Est f(pi)|Est f(pi*)|   Est gap\n",
            "--------------------------------------\n",
            "    5| -2.49e+00| -3.11e+01|  2.86e+01\n",
            "   10|  9.49e+00| -2.06e+01|  3.01e+01\n",
            "   15|  8.71e+00| -1.57e+01|  2.44e+01\n",
            "   20|  8.58e+00| -1.25e+01|  2.10e+01\n",
            "   25|  8.52e+00| -1.00e+01|  1.86e+01\n",
            "   30|  8.54e+00| -8.08e+00|  1.66e+01\n",
            "   35|  8.59e+00| -6.49e+00|  1.51e+01\n",
            "   40|  9.61e+00| -5.12e+00|  1.47e+01\n",
            "   45|  9.91e+00| -3.71e+00|  1.36e+01\n",
            "   50|  7.81e+00| -2.76e+00|  1.06e+01\n",
            "   55|  1.00e+01| -1.93e+00|  1.20e+01\n",
            "   60|  1.08e+01| -1.00e+00|  1.18e+01\n",
            "   65|  1.05e+01| -2.27e-01|  1.07e+01\n",
            "   70|  1.03e+01|  4.08e-01|  9.89e+00\n",
            "   75|  1.02e+01|  9.62e-01|  9.24e+00\n",
            "   80|  1.02e+01|  1.46e+00|  8.77e+00\n",
            "   85|  1.18e+01|  1.93e+00|  9.91e+00\n",
            "   90|  1.00e+01|  2.36e+00|  7.69e+00\n",
            "   95|  9.86e+00|  2.71e+00|  7.15e+00\n",
            "  100|  1.01e+01|  3.05e+00|  7.08e+00\n",
            "Total runtime: 2.04s\n",
            "=== Final performance metric ===\n",
            "  f(pi_k):   8.2997e+00\n",
            "  f(pi*) lb: 3.5252e+00\n",
            "  Gap:       4.7745e+00\n",
            "========================================\n",
            "|---------------------------------------|\n",
            "| : :T: : : : : : :T:T: : : : : : :T: : |\n",
            "| : : : : : : : : : : : : : : : :T:T: :T|\n",
            "| :T:T: : : : : : : : : :T: :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : : :T:T:T|\n",
            "|T: : : : : : :T: : : : : :T: : : : : : |\n",
            "| : :T: : : : :T: :T: : : : : : : : :T: |\n",
            "| : : :T:T: : : : : : : :T:T: : : : : : |\n",
            "| :T: : : : : :T: : : : : : : : : : : : |\n",
            "| : : :T: :T: : : : : : :T:T: : :T: :T: |\n",
            "| : : : :T: : : : : : : : : : : : : : : |\n",
            "| : : : :T: : : : :T: : : : : : :T: : :T|\n",
            "| : : : : : : : : : : : : : : :D: : : :T|\n",
            "| : : : :T: : : : : : : : : : : : : : : |\n",
            "| : : :T: : : : : : : : : : :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : :T: : : |\n",
            "| : :*: : :T: : : : : : : : : : : : : : |\n",
            "| : : : : : : : : : : : : : : : :T: : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "| : :T: : : : :T: : : : :T: : : : : : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "|---------------------------------------|\n",
            "Agent stalled, resetting\n",
            "Target reached in 6 steps! Resetting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just online learning"
      ],
      "metadata": {
        "id": "lVeQxtOwMsLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "settings = dict({\n",
        "      \"alpha\": 1,\n",
        "      \"visual\": \"store_true\",\n",
        "      \"N\": 1,\n",
        "      \"T\": 500,\n",
        "      \"gamma\": 0.9,\n",
        "      \"env_name\": 'gridworld',\n",
        "      \"n_iters\": 100,\n",
        "      \"seed\": 0, # fixed seed so that i get the same env each time\n",
        "      \"advantage\": \"mc\",\n",
        "      \"divergence_shape\": 1.0\n",
        "  })\n",
        "pi_t = online_train(settings, env, pi100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uRblfZkIzcf",
        "outputId": "65bce762-d766-4f30-bcef-230eea3909e4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Iter| Est f(pi)|Est f(pi*)|   Est gap\n",
            "--------------------------------------\n",
            "    5|  5.58e+00| -4.03e+01|  4.59e+01\n",
            "   10|  5.66e+00| -3.68e+01|  4.25e+01\n",
            "   15|  5.73e+00| -3.37e+01|  3.95e+01\n",
            "   20|  5.32e+00| -3.17e+01|  3.70e+01\n",
            "   25|  5.93e+00| -2.96e+01|  3.56e+01\n",
            "   30|  5.77e+00| -2.84e+01|  3.42e+01\n",
            "   35|  5.22e+00| -2.72e+01|  3.24e+01\n",
            "   40|  5.46e+00| -2.62e+01|  3.17e+01\n",
            "   45|  5.78e+00| -2.57e+01|  3.15e+01\n",
            "   50|  5.82e+00| -2.50e+01|  3.08e+01\n",
            "   55|  5.68e+00| -2.45e+01|  3.02e+01\n",
            "   60|  5.76e+00| -2.41e+01|  2.99e+01\n",
            "   65|  6.12e+00| -2.37e+01|  2.99e+01\n",
            "   70|  6.26e+00| -2.35e+01|  2.98e+01\n",
            "   75|  6.47e+00| -2.33e+01|  2.98e+01\n",
            "   80|  6.16e+00| -2.29e+01|  2.91e+01\n",
            "   85|  5.87e+00| -2.24e+01|  2.83e+01\n",
            "   90|  5.82e+00| -2.18e+01|  2.76e+01\n",
            "   95|  5.89e+00| -2.12e+01|  2.71e+01\n",
            "  100|  5.96e+00| -2.05e+01|  2.64e+01\n",
            "Total runtime: 312.88s\n",
            "=== Final performance metric ===\n",
            "  f(pi_k):   6.0238e+00\n",
            "  f(pi*) lb: 4.3277e+00\n",
            "  Gap:       1.6961e+00\n",
            "========================================\n",
            "|---------------------------------------|\n",
            "| : :T: : : : : : :T:T: : : : : : :T: : |\n",
            "| : : : : : : : : : : : : : : : :T:T: :T|\n",
            "| :T:T: : : : : : : : : :T: :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : : :T:T:T|\n",
            "|T: :*: : : : :T: : : : : :T: : : : : : |\n",
            "| : :T: : : : :T: :T: : : : : : : : :T: |\n",
            "| : : :T:T: : : : : : : :T:T: : : : : : |\n",
            "| :T: : : : : :T: : : : : : : : : : : : |\n",
            "| : : :T: :T: : : : : : :T:T: : :T: :T: |\n",
            "| : : : :T: : : : : : : : : : : : : : : |\n",
            "| : : : :T: : : : :T: : : : : : :T: : :T|\n",
            "| : : : : : : : : : : : : : : :D: : : :T|\n",
            "| : : : :T: : : : : : : : : : : : : : : |\n",
            "| : : :T: : : : : : : : : : :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : :T: : : |\n",
            "| : : : : :T: : : : : : : : : : : : : : |\n",
            "| : : : : : : : : : : : : : : : :T: : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "| : :T: : : : :T: : : : :T: : : : : : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "|---------------------------------------|\n",
            "Target reached in 32 steps! Resetting\n",
            "Target reached in 14 steps! Resetting\n",
            "Target reached in 16 steps! Resetting\n",
            "Target reached in 7 steps! Resetting\n",
            "Target reached in 12 steps! Resetting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "generative after online"
      ],
      "metadata": {
        "id": "vvxUfjNNMw-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "settings = dict({\n",
        "      \"alpha\": 1,\n",
        "      \"visual\": \"store_true\",\n",
        "      \"N\": 1,\n",
        "      \"T\": 50,\n",
        "      \"gamma\": 0.9,\n",
        "      \"env_name\": 'gridworld',\n",
        "      \"n_iters\": 100,\n",
        "      \"seed\": 0, # fixed seed so that i get the same env each time\n",
        "      \"advantage\": \"generative\",\n",
        "      \"divergence_shape\": 1.0\n",
        "  })\n",
        "pi_t2 = online_train(settings, env, pi_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0k8N9RgJ9-o",
        "outputId": "6a7c91df-3f53-47b2-cd2e-c0dcd9640cd3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Iter| Est f(pi)|Est f(pi*)|   Est gap\n",
            "--------------------------------------\n",
            "    5|  8.04e+00| -1.39e+00|  9.43e+00\n",
            "   10|  7.41e+00|  1.78e+00|  5.63e+00\n",
            "   15|  6.92e+00|  2.62e+00|  4.30e+00\n",
            "   20|  6.54e+00|  3.18e+00|  3.36e+00\n",
            "   25|  6.56e+00|  3.51e+00|  3.04e+00\n",
            "   30|  6.24e+00|  3.71e+00|  2.52e+00\n",
            "   35|  6.02e+00|  3.85e+00|  2.17e+00\n",
            "   40|  5.96e+00|  3.94e+00|  2.02e+00\n",
            "   45|  5.79e+00|  4.05e+00|  1.74e+00\n",
            "   50|  5.79e+00|  4.15e+00|  1.64e+00\n",
            "   55|  5.57e+00|  4.20e+00|  1.37e+00\n",
            "   60|  5.58e+00|  4.24e+00|  1.34e+00\n",
            "   65|  5.61e+00|  4.28e+00|  1.33e+00\n",
            "   70|  5.57e+00|  4.34e+00|  1.23e+00\n",
            "   75|  5.62e+00|  4.38e+00|  1.25e+00\n",
            "   80|  5.38e+00|  4.42e+00|  9.58e-01\n",
            "   85|  5.52e+00|  4.46e+00|  1.06e+00\n",
            "   90|  5.67e+00|  4.49e+00|  1.18e+00\n",
            "   95|  5.55e+00|  4.53e+00|  1.02e+00\n",
            "  100|  5.51e+00|  4.57e+00|  9.43e-01\n",
            "Total runtime: 47.77s\n",
            "=== Final performance metric ===\n",
            "  f(pi_k):   5.5089e+00\n",
            "  f(pi*) lb: 5.1329e+00\n",
            "  Gap:       3.7597e-01\n",
            "========================================\n",
            "|---------------------------------------|\n",
            "| : :T: : : : : : :T:T: : : : : : :T: : |\n",
            "| : : : : : : : : : : : : : : : :T:T: :T|\n",
            "| :T:T: : : : : : : : : :T: :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : : :T:T:T|\n",
            "|T: : : : : : :T: : : : : :T: : : : : : |\n",
            "| : :T: : : : :T: :T: : : : : : : : :T: |\n",
            "| : : :T:T: : : : : : : :T:T: : : : : : |\n",
            "| :T: : : : : :T: : : : : : : : : : : : |\n",
            "| : : :T: :T: : : : : : :T:T: : :T: :T: |\n",
            "| : : : :T: : : : : : : : : : : : : : : |\n",
            "| : : : :T: : : : :T: : : : : : :T: : :T|\n",
            "| : : : : : : : : : : : : : : :D: : : :T|\n",
            "| : : : :T: : : : : : : : : : : : : : : |\n",
            "| : : :T: : : : : : : : : : :T: : : : : |\n",
            "| : : : : : : : : : : :T: : : : :T: : : |\n",
            "| : : : : :T: : : : : : : : : : : : : : |\n",
            "| : : : : : : : : : : : : : : : :T: : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "| : :T:*: : : :T: : : : :T: : : : : : : |\n",
            "| : : : : : : : : : : : : : : : : : : : |\n",
            "|---------------------------------------|\n",
            "Target reached in 19 steps! Resetting\n",
            "Target reached in 7 steps! Resetting\n",
            "Target reached in 16 steps! Resetting\n",
            "Target reached in 9 steps! Resetting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0tCLPR7bM3aU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}